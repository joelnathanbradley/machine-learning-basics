{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params_deep(layer_widths):\n",
    "    np.random.seed(3)\n",
    "    params = {}\n",
    "    L = len(layer_widths)\n",
    "    assert(L >= 2)\n",
    "    for l in range(1, L-1):\n",
    "        params['w' + str(l)] = np.random.randn(layer_widths[l], layer_widths[l-1])*np.sqrt(2/layer_widths[l-1])\n",
    "        params['b' + str(l)] = np.zeros((layer_widths[l], 1))\n",
    "    params['w' + str(L-1)] = np.random.randn(layer_widths[L-1], layer_widths[L-2])*np.sqrt(1/layer_widths[L-2])\n",
    "    params['b' + str(L-1)] = np.zeros((layer_widths[L-1], 1))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(a_prev, w, b):\n",
    "    z = w.dot(a_prev) + b\n",
    "    linear_cache = (a_prev, w, b)\n",
    "    return z, linear_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_forward(z, activation):\n",
    "    assert(activation == 'softmax' or activation == 'relu')\n",
    "    if activation == 'softmax':\n",
    "        a = np.exp(z) / np.exp(z).sum()\n",
    "    else:\n",
    "        a = np.maximum(z, 0)\n",
    "    return a, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(a_prev, w, b, activation):\n",
    "    z, linear_cache = linear_forward(a_prev, w, b)\n",
    "    a, activation_cache = activation_forward(z, activation)\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return a, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_deep(X, params):\n",
    "    caches = []\n",
    "    a_prev = X\n",
    "    L = len(params) // 2\n",
    "    for layer in range(1, L):\n",
    "        w = params['w' + str(layer)]\n",
    "        b = params['b' + str(layer)]\n",
    "        a, cache = forward(a_prev, w, b, 'relu')\n",
    "        caches.append(cache)\n",
    "        a_prev = a\n",
    "    w = params['w' + str(L)]\n",
    "    b = params['b' + str(L)]\n",
    "    a, cache = forward(a_prev, w, b, 'softmax')\n",
    "    caches.append(cache)\n",
    "    return a, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_backward(da, activation_cache, activation):\n",
    "    assert(activation == 'softmax' or activation == 'relu')\n",
    "    if activation == 'softmax':\n",
    "        dz = da\n",
    "        return dz\n",
    "    else:\n",
    "        z = activation_cache\n",
    "        dz = np.array(da, copy=True)\n",
    "        dz[z > 0] = 1\n",
    "        return dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dz, linear_cache):\n",
    "    a_prev, w, b = linear_cache\n",
    "    m = a_prev.shape[1]\n",
    "    dw = (1/m)*dz.dot(a_prev.T)\n",
    "    db = (1/m)*np.sum(dz, axis=1, keepdims=True)\n",
    "    da_prev = w.T.dot(dz)\n",
    "    return da_prev, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(da, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    dz = activation_backward(da, activation_cache, activation)\n",
    "    da_prev, dw, db = linear_backward(dz, linear_cache)\n",
    "    return da_prev, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_deep(a, y, caches):\n",
    "    gradients = {}\n",
    "    L = len(caches)\n",
    "    m = a.shape[1]\n",
    "    dz = a - y\n",
    "    current_cache = caches[L-1]\n",
    "    da_prev, dw, db = backward(dz, current_cache, 'softmax')\n",
    "    gradients['da' + str(L-1)] = da_prev\n",
    "    gradients['dw' + str(L)] = dw\n",
    "    gradients['db' + str(L)] = db\n",
    "    a = a_prev\n",
    "    for layer in reversed(range(L-1)):\n",
    "        current_cache = caches[layer]\n",
    "        a_prev, dw, db = backward(a, current_cache, 'relu')\n",
    "        gradients['da' + str(layer)] = a_prev\n",
    "        gradients['dw' + str(layer+1)] = dw\n",
    "        gradients['db' + str(layer+1)] = db\n",
    "        a = a_prev\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnn_model(X, y, alpha=0.001, epochs=5000, verbose=False):\n",
    "    m = X.shape[1]\n",
    "    input_dim = X.shape[0]\n",
    "    output_dim = y.shape[0]\n",
    "    layer_widths = [input_dim, 100, 100, 100, 100, 100, output_dim]\n",
    "    params = init_params_deep(layer_widths)\n",
    "    for epoch in range(epochs):\n",
    "        a, cache = forward_deep(X, params)\n",
    "        gradients = backward_deep(a, y, params, cache)\n",
    "        #update parameters\n",
    "        #compute cross entropy cost\n",
    "    # return parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
